{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dropout",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8447c93cbd2d4c02a9bc1a249d82d694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f7481b0920ad4ffd9b19cabbd2893c74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_dbf4aefdf9614d0082ef5ef7e862b64e",
              "IPY_MODEL_5d66a50899e94b35996c0664ed95f406"
            ]
          }
        },
        "f7481b0920ad4ffd9b19cabbd2893c74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dbf4aefdf9614d0082ef5ef7e862b64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_be018038c02148d3bac2dd2d33a18a3b",
            "_dom_classes": [],
            "description": " 11%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 100001,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 11377,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e78cecd899be4d389a813ba32686a50d"
          }
        },
        "5d66a50899e94b35996c0664ed95f406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_feebe595a10248c786d604084dc81cab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 11377/100001 [1:49:56&lt;14:15:28,  1.73it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_853327c79b31492f8b70177d3433cc7f"
          }
        },
        "be018038c02148d3bac2dd2d33a18a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e78cecd899be4d389a813ba32686a50d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "feebe595a10248c786d604084dc81cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "853327c79b31492f8b70177d3433cc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "flIul56sqEw-",
        "outputId": "67f9c4af-d276-48d6-be5d-fff39f933d38"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('/kaggle/input/lyft-motion-prediction-autonomous-vehicles'):\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import zipfile\n",
        "    !pip install -q kaggle\n",
        "    \n",
        "    from google.colab import files\n",
        "    if not os.path.exists('kaggle.json'):\n",
        "        files.upload()\n",
        "        !mkdir ~/.kaggle\n",
        "        !cp kaggle.json ~/.kaggle/\n",
        "        !chmod 600 ~/.kaggle/kaggle.json\n",
        "    #!kaggle datasets list\n",
        "    !kaggle config path -p /content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def kaggle_dataset_download_unzip(lfilename,dir):\n",
        "\n",
        "        with zipfile.ZipFile(lfilename) as file: \n",
        "            file.extractall(dir)\n",
        "        os.remove(lfilename)\n",
        "\n",
        "    dir='/content/lyft-motion'\n",
        "    if not os.path.exists(dir):\n",
        "        !kaggle datasets download -d aikhmelnytskyy/liftfirst\n",
        "        lfilename='/content/liftfirst.zip'\n",
        "        kaggle_dataset_download_unzip(lfilename,dir+'/scenes')\n",
        "\n",
        "        !kaggle datasets download -d aikhmelnytskyy/lyftsecond\n",
        "        lfilename='/content/lyftsecond.zip'\n",
        "        kaggle_dataset_download_unzip(lfilename,dir)\n",
        "\n",
        "        !kaggle datasets download -d huanvo/lyft-pretrained-model-hv\n",
        "        lfilename='/content/lyft-pretrained-model-hv.zip'\n",
        "        kaggle_dataset_download_unzip(lfilename,'/content/lyft-pretrained-model-hv')\n",
        "        \n",
        "        data_path='/content/lyft-motion/'\n",
        "        weight_path='/content/lyft-pretrained-model-hv/model_multi_update_lyft_public.pth'\n",
        "        model_path='/content/drive/My Drive/Models/'#!!!!!!!!!!\n",
        "else:\n",
        "    data_path='/kaggle/input/lyft-motion-prediction-autonomous-vehicles'\n",
        "    weight_path='/kaggle/input/lyft-pretrained-model-hv/model_multi_update_lyft_public.pth' \n",
        "    model_path='' "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-38d5de13-4ddc-4f44-b708-ecb93d99bae4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-38d5de13-4ddc-4f44-b708-ecb93d99bae4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "usage: kaggle config [-h] {view,set,unset} ...\n",
            "kaggle config: error: argument command: invalid choice: 'path' (choose from 'view', 'set', 'unset')\n",
            "Downloading liftfirst.zip to /content\n",
            "100% 15.9G/15.9G [03:16<00:00, 71.5MB/s]\n",
            "100% 15.9G/15.9G [03:16<00:00, 87.1MB/s]\n",
            "Downloading lyftsecond.zip to /content\n",
            "100% 2.33G/2.34G [00:15<00:00, 145MB/s]\n",
            "100% 2.34G/2.34G [00:15<00:00, 166MB/s]\n",
            "Downloading lyft-pretrained-model-hv.zip to /content\n",
            " 93% 133M/143M [00:01<00:00, 97.8MB/s]\n",
            "100% 143M/143M [00:01<00:00, 114MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwmOFEGht3Nn",
        "outputId": "a46dda3d-5f77-4016-fa1b-d60ca9c1bb79"
      },
      "source": [
        "# this script transports l5kit and dependencies\n",
        "!pip -q install pymap3d==2.1.0 \n",
        "!pip -q install protobuf==3.12.2 \n",
        "!pip -q install transforms3d \n",
        "!pip -q install zarr \n",
        "!pip -q install ptable\n",
        "\n",
        "!pip -q install --no-dependencies l5kit"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for pymap3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3MB 4.3MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 71kB 3.6MB/s \n",
            "\u001b[?25h  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133kB 4.4MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.8MB 6.7MB/s \n",
            "\u001b[?25h  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ptable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 92kB 4.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-78QUb6qFuX",
        "outputId": "4d79466c-a264-4bf3-e1eb-3e5d2d470127"
      },
      "source": [
        "import l5kit\n",
        "print(l5kit.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wPJVD3St5X_"
      },
      "source": [
        "# import packages\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import torch\n",
        "import gc, os\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models.resnet import resnet18, resnet34, resnet50\n",
        "from torchvision.models.densenet import densenet121\n",
        "from tqdm import tqdm\n",
        "from typing import Dict\n",
        "\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jdBfx8Qt60d"
      },
      "source": [
        "from typing import Dict\n",
        "\n",
        "from tempfile import gettempdir\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "# from torchvision.models.resnet import resnet50, resnet18, resnet34, resnet101\n",
        "from tqdm import tqdm\n",
        "\n",
        "import l5kit\n",
        "from l5kit.configs import load_config_data\n",
        "from l5kit.data import LocalDataManager, ChunkedDataset\n",
        "from l5kit.dataset import AgentDataset, EgoDataset\n",
        "from l5kit.rasterization import build_rasterizer\n",
        "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
        "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
        "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\n",
        "from l5kit.geometry import transform_points\n",
        "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
        "from prettytable import PrettyTable\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm_notebook\n",
        "import gc, psutil\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOEgRzT2t8Im",
        "outputId": "b362dff3-9db6-4a37-afe4-a6d19e51d134"
      },
      "source": [
        "# Memory measurement\n",
        "def memory(verbose=True):\n",
        "    mem = psutil.virtual_memory()\n",
        "    gb = 1024*1024*1024\n",
        "    if verbose:\n",
        "        print('Physical memory:',\n",
        "              '%.2f GB (used),'%((mem.total - mem.available) / gb),\n",
        "              '%.2f GB (available)'%((mem.available) / gb), '/',\n",
        "              '%.2f GB'%(mem.total / gb))\n",
        "    return (mem.total - mem.available) / gb\n",
        "\n",
        "def gc_memory(verbose=True):\n",
        "    m = gc.collect()\n",
        "    if verbose:\n",
        "        print('GC:', m, end=' | ')\n",
        "        memory()\n",
        "\n",
        "memory();"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Physical memory: 1.08 GB (used), 24.43 GB (available) / 25.51 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Amj8uPJt9c7"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "set_seed(42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX52mNsNt_AP"
      },
      "source": [
        "# --- Lyft configs ---\n",
        "cfg = {\n",
        "    'format_version': 4,\n",
        "    'data_path': '/content/lyft-motion',\n",
        "    'model_params': {\n",
        "        'model_architecture': 'resnet34',\n",
        "        'history_num_frames': 10,\n",
        "        'history_step_size': 1,\n",
        "        'history_delta_time': 0.1,\n",
        "        'future_num_frames': 50,\n",
        "        'future_step_size': 1,\n",
        "        'future_delta_time': 0.1,\n",
        "        'model_name': \"model_resnet34_wegihted_retrain_baded_on_235000\",\n",
        "        'lr': 5e-6,\n",
        "        'weight_path': '/content/drive/MyDrive/dropout03/model_resnet34_wegihted_output_235000.pth',\n",
        "        'train': True,\n",
        "        'predict': False,\n",
        "    },\n",
        "    'raster_params': {\n",
        "        'raster_size': [224, 224],\n",
        "        'pixel_size': [0.5, 0.5],\n",
        "        'ego_center': [0.25, 0.5],\n",
        "        'map_type': 'py_semantic',\n",
        "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
        "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
        "        'dataset_meta_key': 'meta.json',\n",
        "        'filter_agents_threshold': 0.5,\n",
        "    },\n",
        "    'train_data_loader': {\n",
        "        'key': 'scenes/train.zarr',\n",
        "        'batch_size': 8,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 1,\n",
        "    },    \n",
        "    'test_data_loader': {\n",
        "        'key': 'scenes/test.zarr',\n",
        "        'batch_size': 128,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 4,\n",
        "    },\n",
        "    'train_params': {\n",
        "        # 'steps': 100,\n",
        "        # 'update_steps': 10,\n",
        "        # 'checkpoint_steps': 50,\n",
        "        'steps': 100000,\n",
        "        'update_steps': 200,\n",
        "        'checkpoint_steps': 20000,\n",
        "    }\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59EoiZ7MuBSZ"
      },
      "source": [
        "# set env variable for data\n",
        "DIR_INPUT = cfg[\"data_path\"]\n",
        "os.environ[\"L5KIT_DATA_FOLDER\"] = DIR_INPUT\n",
        "dm = LocalDataManager()"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePgw67zouJ-E"
      },
      "source": [
        "# --- Function utils ---\n",
        "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "def pytorch_neg_multi_log_likelihood_batch(\n",
        "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "    Compute a negative log-likelihood for the multi-modal scenario.\n",
        "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
        "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
        "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
        "    https://leimao.github.io/blog/LogSumExp/\n",
        "    Args:\n",
        "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
        "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
        "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
        "    Returns:\n",
        "        Tensor: negative log-likelihood for this example, a single float number\n",
        "    \"\"\"\n",
        "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
        "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
        "\n",
        "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
        "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
        "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
        "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
        "    # assert all data are valid\n",
        "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
        "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
        "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
        "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
        "\n",
        "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
        "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
        "    avails = avails[:, None, :, None]  # add modes and cords\n",
        "\n",
        "    # error (batch_size, num_modes, future_len)\n",
        "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
        "\n",
        "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
        "        # error (batch_size, num_modes)\n",
        "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
        "\n",
        "    # use max aggregator on modes for numerical stability\n",
        "    # error (batch_size, num_modes)\n",
        "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
        "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
        "    # print(\"error\", error)\n",
        "    return torch.mean(error)\n",
        "\n",
        "\n",
        "def pytorch_neg_multi_log_likelihood_single(\n",
        "    gt: Tensor, pred: Tensor, avails: Tensor\n",
        ") -> Tensor:\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
        "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
        "    Returns:\n",
        "        Tensor: negative log-likelihood for this example, a single float number\n",
        "    \"\"\"\n",
        "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
        "    # create confidence (bs)x(mode=1)\n",
        "    batch_size, future_len, num_coords = pred.shape\n",
        "    confidences = pred.new_ones((batch_size, 1))\n",
        "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJTwariTuQPC"
      },
      "source": [
        "class LyftMultiModel(nn.Module):\n",
        "    def __init__(self, cfg: Dict, num_modes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        architecture = cfg[\"model_params\"][\"model_architecture\"]\n",
        "        backbone = eval(architecture)(pretrained=True, progress=True)\n",
        "        self.backbone = backbone\n",
        "\n",
        "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
        "        num_in_channels = 3 + num_history_channels\n",
        "\n",
        "        self.backbone.conv1 = nn.Conv2d(\n",
        "            num_in_channels,\n",
        "            self.backbone.conv1.out_channels,\n",
        "            kernel_size=self.backbone.conv1.kernel_size,\n",
        "            stride=self.backbone.conv1.stride,\n",
        "            padding=self.backbone.conv1.padding,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "        # This is 512 for resnet18 and resnet34\n",
        "        # And it is 2048 for the other resnets        \n",
        "        if architecture == \"resnet50\":\n",
        "            backbone_out_features = 2048\n",
        "        else:\n",
        "            backbone_out_features = 512\n",
        "        \n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        # X, Y coords for the future positions (output shape: batch_sizex50x2)\n",
        "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
        "        num_targets = 2 * self.future_len\n",
        "\n",
        "        # You can add more layers here.\n",
        "        self.head = nn.Sequential(\n",
        "            # nn.Dropout(0.2),\n",
        "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
        "        )\n",
        "\n",
        "        self.num_preds = num_targets * num_modes\n",
        "        self.num_modes = num_modes\n",
        "\n",
        "        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone.conv1(x)\n",
        "        x = self.backbone.bn1(x)\n",
        "        x = self.backbone.relu(x)\n",
        "        x = self.backbone.maxpool(x)\n",
        "\n",
        "        x = self.backbone.layer1(x)\n",
        "        x = self.backbone.layer2(x)\n",
        "        x = self.backbone.layer3(x)\n",
        "        x = self.backbone.layer4(x)\n",
        "\n",
        "        x = self.backbone.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.head(x)\n",
        "        x = self.logit(x)\n",
        "\n",
        "        # pred (batch_size)x(modes)x(time)x(2D coords)\n",
        "        # confidences (batch_size)x(modes)\n",
        "        bs, _ = x.shape\n",
        "        pred, confidences = torch.split(x, self.num_preds, dim=1)\n",
        "        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n",
        "        assert confidences.shape == (bs, self.num_modes)\n",
        "        confidences = torch.softmax(confidences, dim=1)\n",
        "        return pred, confidences"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AbOrFeRuRbW"
      },
      "source": [
        "\n",
        "def forward(data, model, device, criterion=pytorch_neg_multi_log_likelihood_batch, compute_loss=True):\n",
        "    inputs = data[\"image\"].to(device)\n",
        "    target_availabilities = data[\"target_availabilities\"].to(device)\n",
        "    targets = data[\"target_positions\"].to(device)\n",
        "    # Forward pass\n",
        "    preds, confidences = model(inputs)\n",
        "    # skip compute loss if we are doing prediction\n",
        "    loss = criterion(targets, preds, confidences, target_availabilities) if compute_loss else 0\n",
        "    return loss, preds, confidences"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0PMl09wuSjM",
        "outputId": "03cd0120-cdf0-4153-9d72-57d7d5c09d2d"
      },
      "source": [
        "%%time\n",
        "# Build rasterizer\n",
        "rasterizer = build_rasterizer(cfg, dm)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.41 s, sys: 353 ms, total: 5.76 s\n",
            "Wall time: 5.41 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIyeCFXMuU9i",
        "outputId": "41374445-65ee-473b-bc6c-633ef61dd691"
      },
      "source": [
        "%%time\n",
        "# Train dataset\n",
        "train_cfg = cfg[\"train_data_loader\"]\n",
        "train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open(cached=False)  # to prevent run out of memory\n",
        "train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], \n",
        "                              batch_size=train_cfg[\"batch_size\"], num_workers=train_cfg[\"num_workers\"])\n",
        "print(train_dataset)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "CPU times: user 6.38 s, sys: 3.43 s, total: 9.81 s\n",
            "Wall time: 9.54 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1V0sfTIuWSf"
      },
      "source": [
        "# samples_weight = pd.read_csv(\"/content/drive/MyDrive/baseline improvements/w_traindata.csv\")\n",
        "# samples_weight = samples_weight[[\"0\"]].to_numpy()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gar1NvkuZkB",
        "outputId": "b4b00c6d-86d7-4168-b8d7-883d2d5f1f34"
      },
      "source": [
        "%%time\n",
        "if cfg[\"model_params\"][\"train\"]:\n",
        "    # Train dataset\n",
        "    train_cfg = cfg[\"train_data_loader\"]\n",
        "    train_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open(cached=False)  # to prevent run out of memory\n",
        "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
        "    \n",
        "    # samples_weight = torch.from_numpy(samples_weight)\n",
        "    # sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))   \n",
        "    # train_dataloader = DataLoader(train_dataset, shuffle=False, \n",
        "    #                               batch_size=train_cfg[\"batch_size\"], num_workers=train_cfg[\"num_workers\"],sampler=sampler)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], \n",
        "                                  batch_size=train_cfg[\"batch_size\"], num_workers=train_cfg[\"num_workers\"])\n",
        "    print(train_dataset)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "| Num Scenes | Num Frames | Num Agents | Num TR lights | Total Time (hr) | Avg Frames per Scene | Avg Agents per Frame | Avg Scene Time (sec) | Avg Frame frequency |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "|   16265    |  4039527   | 320124624  |    38735988   |      112.19     |        248.36        |        79.25         |        24.83         |        10.00        |\n",
            "+------------+------------+------------+---------------+-----------------+----------------------+----------------------+----------------------+---------------------+\n",
            "CPU times: user 6.17 s, sys: 1.12 s, total: 7.29 s\n",
            "Wall time: 6.97 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyKihvJLuazq",
        "outputId": "9e92e9ad-5a78-4cac-eb37-f4ea2a840122"
      },
      "source": [
        "len(train_dataloader)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2812089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGPl5VRIucTa",
        "outputId": "68bc04b5-5d65-4d69-977a-508661fae032"
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22496709"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocuZnL2xueoH",
        "outputId": "f88858d9-61ee-4a22-98f0-4aab4922cb92"
      },
      "source": [
        "%%time\n",
        "# ==== INIT MODEL=================\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = LyftMultiModel(cfg)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=cfg[\"model_params\"][\"lr\"])\n",
        "\n",
        "#load weight if there is a pretrained model\n",
        "# epoch = 1\n",
        "# WEIGHT_FILE = cfg[\"model_params\"][\"weight_path\"]\n",
        "# checkpoint = torch.load(WEIGHT_FILE, map_location=device)\n",
        "# model.load_state_dict(checkpoint)\n",
        "\n",
        "WEIGHT_FILE = cfg[\"model_params\"][\"weight_path\"]\n",
        "checkpoint = torch.load(WEIGHT_FILE, map_location=device)\n",
        "epoch = checkpoint['epoch']\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "print(f'device {device}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "device cuda:0\n",
            "CPU times: user 849 ms, sys: 333 ms, total: 1.18 s\n",
            "Wall time: 3.72 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIvh14g2updr",
        "outputId": "58f44993-9ca9-4f61-96de-384e17fcf44d"
      },
      "source": [
        "epoch"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8447c93cbd2d4c02a9bc1a249d82d694",
            "f7481b0920ad4ffd9b19cabbd2893c74",
            "dbf4aefdf9614d0082ef5ef7e862b64e",
            "5d66a50899e94b35996c0664ed95f406",
            "be018038c02148d3bac2dd2d33a18a3b",
            "e78cecd899be4d389a813ba32686a50d",
            "feebe595a10248c786d604084dc81cab",
            "853327c79b31492f8b70177d3433cc7f"
          ]
        },
        "id": "ETouzTaSuhqy",
        "outputId": "6835da6a-012f-4dd1-b574-3d403859e0ff"
      },
      "source": [
        "%%time\n",
        "if cfg[\"model_params\"][\"train\"]:\n",
        "    tr_it = iter(train_dataloader)\n",
        "    n_steps = cfg[\"train_params\"][\"steps\"]\n",
        "    progress_bar = tqdm_notebook(range(epoch, 1 + epoch + cfg[\"train_params\"][\"steps\"]), mininterval=10.)\n",
        "    losses = []\n",
        "    iterations = []\n",
        "    metrics = []\n",
        "    memorys = []\n",
        "    times = []\n",
        "    model_name = cfg[\"model_params\"][\"model_name\"]\n",
        "    update_steps = cfg['train_params']['update_steps']\n",
        "    checkpoint_steps = cfg['train_params']['checkpoint_steps']\n",
        "    t_start = time.time()\n",
        "    torch.set_grad_enabled(True)\n",
        "        \n",
        "    for i in progress_bar:\n",
        "        try:\n",
        "            data = next(tr_it)\n",
        "        except StopIteration:\n",
        "            tr_it = iter(train_dataloader)\n",
        "            data = next(tr_it)\n",
        "        model.train()   # somehow we need this is ever batch or it perform very bad (not sure why)\n",
        "        loss, _, _ = forward(data, model, device)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_v = loss.item()\n",
        "        losses.append(loss_v)\n",
        "        \n",
        "        if i % update_steps == 0:\n",
        "            mean_losses = np.mean(losses)\n",
        "            timespent = (time.time() - t_start) / 60\n",
        "            print('i: %5d'%i,\n",
        "                  'loss: %10.5f'%loss_v, 'loss(avg): %10.5f'%mean_losses, \n",
        "                  '%.2fmins'%timespent, end=' | ')\n",
        "            mem = memory()\n",
        "            # if i % checkpoint_steps == 0:\n",
        "            #     torch.save(model.state_dict(), f'{model_name}_{i}.pth')\n",
        "            #     torch.save(optimizer.state_dict(), f'{model_name}_optimizer_{i}.pth')\n",
        "            if i % checkpoint_steps == 0:\n",
        "                torch.save({'epoch': i,\n",
        "                            'model_state_dict': model.state_dict(),\n",
        "                            'optimizer_state_dict': optimizer.state_dict()},\n",
        "                           f'/content/drive/My Drive/dropout03/{model_name}_output_{i}.pth') \n",
        "            iterations.append(i)\n",
        "            metrics.append(mean_losses)\n",
        "            memorys.append(mem)\n",
        "            times.append(timespent)\n",
        "    \n",
        "    results = pd.DataFrame({\n",
        "        'iterations': iterations, \n",
        "        'metrics (avg)': metrics,\n",
        "        'elapsed_time (mins)': times,\n",
        "        'memory (GB)': memorys,\n",
        "    })\n",
        "    results.to_csv(f'/content/drive/My Drive/dropout03/train_metrics_{model_name}_{i}.csv', index=False)\n",
        "    print(f'Total training time is {(time.time() - t_start) / 60} mins')\n",
        "    memory()\n",
        "    display(results)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8447c93cbd2d4c02a9bc1a249d82d694",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=100001.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "i: 235000 loss:   18.95519 loss(avg):   18.95519 0.03mins | Physical memory: 3.88 GB (used), 21.63 GB (available) / 25.51 GB\n",
            "i: 235200 loss:    2.57253 loss(avg):   16.31812 2.24mins | Physical memory: 3.87 GB (used), 21.65 GB (available) / 25.51 GB\n",
            "i: 235400 loss:   11.92922 loss(avg):   16.03517 4.35mins | Physical memory: 3.86 GB (used), 21.65 GB (available) / 25.51 GB\n",
            "i: 235600 loss:   22.03712 loss(avg):   16.28435 6.44mins | Physical memory: 3.99 GB (used), 21.52 GB (available) / 25.51 GB\n",
            "i: 235800 loss:   16.52003 loss(avg):   16.33897 8.45mins | Physical memory: 4.09 GB (used), 21.43 GB (available) / 25.51 GB\n",
            "i: 236000 loss:    4.37050 loss(avg):   15.69369 10.51mins | Physical memory: 4.08 GB (used), 21.43 GB (available) / 25.51 GB\n",
            "i: 236200 loss:    6.53674 loss(avg):   16.05157 12.50mins | Physical memory: 4.13 GB (used), 21.38 GB (available) / 25.51 GB\n",
            "i: 236400 loss:   16.44347 loss(avg):   15.68606 14.52mins | Physical memory: 4.36 GB (used), 21.15 GB (available) / 25.51 GB\n",
            "i: 236600 loss:    3.56978 loss(avg):   15.18193 16.47mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 236800 loss:    2.86072 loss(avg):   15.62715 18.45mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 237000 loss:    7.89802 loss(avg):   15.24249 20.45mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 237200 loss:    2.21016 loss(avg):   14.95674 22.47mins | Physical memory: 4.62 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 237400 loss:    0.95211 loss(avg):   14.59500 24.44mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 237600 loss:   15.90269 loss(avg):   14.50307 26.42mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 237800 loss:    3.37702 loss(avg):   14.69481 28.36mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 238000 loss:   10.51151 loss(avg):   14.55832 30.30mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 238200 loss:   12.69978 loss(avg):   14.52847 32.31mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 238400 loss:    3.98680 loss(avg):   14.42189 34.16mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 238600 loss:    2.62755 loss(avg):   14.26939 36.11mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 238800 loss:    1.46247 loss(avg):   14.30802 38.04mins | Physical memory: 4.62 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 239000 loss:   14.86837 loss(avg):   14.28878 40.01mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 239200 loss:    6.99657 loss(avg):   14.23304 42.00mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 239400 loss:    5.20079 loss(avg):   14.25118 43.91mins | Physical memory: 4.61 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 239600 loss:   16.89041 loss(avg):   14.22873 45.88mins | Physical memory: 4.62 GB (used), 20.89 GB (available) / 25.51 GB\n",
            "i: 239800 loss:   17.16995 loss(avg):   14.18696 47.80mins | Physical memory: 4.62 GB (used), 20.90 GB (available) / 25.51 GB\n",
            "i: 240000 loss:    6.38823 loss(avg):   14.25971 49.70mins | Physical memory: 4.63 GB (used), 20.88 GB (available) / 25.51 GB\n",
            "i: 240200 loss:   11.70800 loss(avg):   14.29598 51.62mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 240400 loss:   22.42062 loss(avg):   14.22836 53.58mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 240600 loss:   25.72759 loss(avg):   14.21119 55.55mins | Physical memory: 4.64 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 240800 loss:   14.38388 loss(avg):   14.17559 57.48mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 241000 loss:    4.14468 loss(avg):   14.11844 59.43mins | Physical memory: 4.64 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 241200 loss:    4.37157 loss(avg):   14.03977 61.36mins | Physical memory: 4.64 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 241400 loss:    6.44047 loss(avg):   14.02874 63.30mins | Physical memory: 4.64 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 241600 loss:    4.85334 loss(avg):   13.99926 65.16mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 241800 loss:   11.19969 loss(avg):   13.97104 67.06mins | Physical memory: 4.64 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 242000 loss:    4.45932 loss(avg):   13.88974 68.97mins | Physical memory: 4.64 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 242200 loss:    5.14997 loss(avg):   13.88723 70.86mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 242400 loss:   11.94155 loss(avg):   13.95240 72.73mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 242600 loss:    4.01481 loss(avg):   13.94777 74.55mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 242800 loss:    3.37023 loss(avg):   13.93422 76.45mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 243000 loss:    9.76741 loss(avg):   13.92549 78.33mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 243200 loss:    8.18784 loss(avg):   13.94889 80.11mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 243400 loss:   10.25699 loss(avg):   13.96931 81.89mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 243600 loss:   22.46070 loss(avg):   13.96857 83.79mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 243800 loss:    1.20006 loss(avg):   13.92765 85.73mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 244000 loss:   12.19998 loss(avg):   13.95001 87.52mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 244200 loss:    9.05199 loss(avg):   13.91310 89.41mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 244400 loss:    8.78905 loss(avg):   13.88417 91.28mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 244600 loss:   23.57302 loss(avg):   13.88475 93.15mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 244800 loss:    7.67259 loss(avg):   13.83820 95.03mins | Physical memory: 4.65 GB (used), 20.87 GB (available) / 25.51 GB\n",
            "i: 245000 loss:    3.12135 loss(avg):   13.85385 96.98mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 245200 loss:   11.16757 loss(avg):   13.83403 98.91mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 245400 loss:    6.78345 loss(avg):   13.82049 100.83mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 245600 loss:    4.28517 loss(avg):   13.85497 102.63mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 245800 loss:    9.41889 loss(avg):   13.84809 104.48mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 246000 loss:   32.52697 loss(avg):   13.85633 106.34mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n",
            "i: 246200 loss:    4.18429 loss(avg):   13.83045 108.26mins | Physical memory: 4.65 GB (used), 20.86 GB (available) / 25.51 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeAFekhJu5AD"
      },
      "source": [
        "if cfg[\"model_params\"][\"train\"]:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(results['iterations'], results['metrics (avg)'])\n",
        "    plt.xlabel('steps'); plt.ylabel('metrics (avg)')\n",
        "    plt.grid(); plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(results['iterations'], results['memory (GB)'])\n",
        "    plt.xlabel('steps'); plt.ylabel('memory (GB)')\n",
        "    plt.grid(); plt.show()\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.plot(results['iterations'], results['elapsed_time (mins)'])\n",
        "    plt.xlabel('steps'); plt.ylabel('elapsed_time (mins)')\n",
        "    plt.grid(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYT_O5rnzoTn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}